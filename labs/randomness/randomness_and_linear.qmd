---
title: "Randomness and the linear model"
author: "Bio Data Science^3"
date: "July 19 - August 2, 2025"
format: 
  html:
    toc: true 
    code-fold: false
    code-tools: true
    embed-resources: true
    highlight-style: github
    code-line-numbers: false 
params:
  skip_execution: false
  skip_answers: true
---

```{r}
#| label: initialize
#| echo: FALSE
knitr::opts_chunk$set(echo = TRUE, fig.width=7, fig.height=5) 
```

## Introduction

In this document, we'll explore when we have data which is spread randomly around a linear relationship, e.g. pairs $x_i, y_i$ in 2D where the $y_i$ are scattered around a line. Often when we think of linear regression, we imagine one $x$ and one $y$ variable (here writing them as vectors), but in general a linear model implies using a linear combination of original predictor variables $(x^1, x^2, \dots, x^p)$ or set of transformed predictor variables, to account for variance in a response $y$. We can say then that $y$ falls near a "linear subspace" that may be spanned by a number of vectors $x$. We can refer to the $x$ vectors as predictors, covariates, or features, and the $y$ may be called called the outcome, the observations, or the target, depending on context and field.

## Random data around a line

$$y = a + b x + \varepsilon$$

the above type of notation will be shorthand in this lecture for write that for all $i \in 1, \dots, n$:

$$
y_i = a + b x_i + \varepsilon_i
$$

$\varepsilon_i$ is "extra" beyond the linear relationship. You can hear it referred to as "error" or "epsilon" (the Greek name for the symbol we use). Note that, since we don't know $a$ or $b$ we don't "observe" $\varepsilon_i$ but will have to estimate it.

We will assume $E(\varepsilon_i) = 0$ , $\textrm{Var}(\varepsilon_i) = \sigma^2$ for all $i$, and $\textrm{Cov}(\varepsilon_i, \varepsilon_j) = 0$ for all $i,j$. We don't need distributional assumptions just yet. We will see we get a lot of useful properties from fitted linear models with just these three assumptions.

Note that the first assumption also implies $E(y) = a + b x$, that the conditional mean of $y$ is linear with $x$.

::: {.callout-note collapse="false"}
## Question

What does it mean here "extra"? Is it "error", "noise", something else?
:::

```{r}
set.seed(5)
n <- 1000
x <- rnorm(n, sd=0.5)
head(x)
hist(x)
var(x)
x <- qnorm(p=1:n/(n+1), sd=0.5)
head(x)
hist(x)
var(x)
x <- rep(0:1, length.out=n)
head(x)
var(x)
```

```{r}
n <- 20
extra_sd <- 1
extra <- rnorm(n, sd=extra_sd)
a <- 3
x <- rep(0:1, each=n/2)
b <- 0
y <- a + b * x + extra
plot(x, y)
extra_sd <- 0.1
extra <- rnorm(n, sd=extra_sd)
b <- 2
y <- a + b * x + extra
plot(x, y)
```

::: {.callout-note collapse="false"}
## Question

a and b are unknown, x is known. But they are considered "fixed" in the world of our model. None are considered "random". What are the implications of these assumptions?
:::

$\varepsilon$ is also not known, need to estimate a, b, $\varepsilon$ using x and y

## Estimation of the linear coefficients and "extra"

Let's move to the general case where we have more than one $x$, we will capitalize to represent the matrix $X$ and change to using `X` in the code. We can use `X %*% b` with matrix `X` and vector `b` to accomplish a linear combinations of the component columns $x^1$ and $x^2$ of $X$.

We will also switch by convention to referring to the vector in code `b` as a vector variable $\beta$, and estimates of b to a random vector $\hat{\beta}$.

::: {.callout-note collapse="false"}
## Question

Why is $\hat{\beta}$ random, if we said $\beta$ is fixed?
:::

```{r}
x1 <- rnorm(n, sd=0.5)
x2 <- rnorm(n, sd=0.5)
X <- cbind(x1, x2)
extra_sd <- 1
extra <- rnorm(n, sd = extra_sd)
a <- 3
b <- c(2, 1)
y <- a + X %*% b + extra
# simpler, and equivalent:
X <- cbind(intercept=rep(1,n), x1, x2)
b <- c(3, 2, 1)
y <- X %*% b + extra 
```

We will use $p$ to denote the number of columns of $X$. Sometimes you will see $p + 1$ for the number of columns of $X$ if the coefficients are counted from $\beta_0, \beta_1, \dots, \beta_p$ with $\beta_0$ as the intercept (e.g. `a` above).

We can show that for linear models with an intercept, there are a number of properties, such as the property that the residuals (predictions minus observed outcomes) will sum to 0. For models where the $x$'s are "dummy variables" (indicator variables taking values $\{0,1\}$) we will have $\hat{\beta}_0 = 1/n_0 \sum_{i:x_i = 0} y_i$ , that is, the estimated coefficient will be equal to the mean value of the observed outcome when the predictors are all equal to 0. If there's no data point where all $x_i=0$, the intercept becomes an extrapolation to a fitted value where $x_i = 0$, not a literal average of data.

### Finding solution with calculus

We want to find $\hat{\beta}$, $\hat{\sigma}^2$ that are optimal in some sense. One criterion is to minimize the squared differences from observed $y$ to predicted $\hat{y}$ ("least squares" or sometimes "ordinary least squares", OLS). We compute per observation "residuals", $\hat{\varepsilon}_i = \hat{y}_i - y_i$. Below I will write $\hat{\varepsilon}$ as the vector of residuals for all $n$ observations.

::: {.callout-note collapse="false"}
## Question

Why consider square of the residuals? What are squares good for in calculus, and what do they correspond to in geometry? Why is this a good idea, and why maybe a bad idea?
:::

Total squared distance/error of prediction, also called the residual sum of squares, can be computed and simplified into terms involving $\hat{\beta}$. Note here we refer to *vectors* $\varepsilon, y, \hat{y}, \hat{\beta}$ and matrix $X$.

$$
\begin{align}
\hat{\varepsilon}' \hat{\varepsilon} &= (y-\hat{y})'(y-\hat{y}) \\
&= (y-X \hat{\beta})'(y-X \hat{\beta}) \\
&= y' y - 2 \hat{\beta}' X' y + \hat{\beta}' X' X \hat{\beta}
\end{align}
$$

Taking derivatives with respect to (vector) $\hat{\beta}$ and setting equal to 0

$$
0 = -2X'y + 2X'X \hat{\beta}
$$

This gives the "normal equations" (here shown with dimensions as subscripts)

$$
(X'X)_{p \times p} ~ \hat{\beta}_{p \times 1}=X'_{p \times n} ~y_{n \times 1} 
$$

$\hat{{\beta}}$ that solves the normal equations gives us $\hat{y} = X \hat{{\beta}}$ and residuals $\hat{\varepsilon}$ such that

$$
\hat{y}' \hat{\varepsilon} = 0
$$

The predicted values and residuals are orthogonal. We will investigate this further in the next section.

::: {.callout-note collapse="false"}
## Question

Does this make sense? What if the predicted values and residuals were correlated? E.g. imagine whenever predicted $\hat{y}$ is positive, the residual is positive, and whenever predicted $\hat{y}$ is negative the residual is negative.
:::

### Geometric intuition of the normal equations

We seek $\hat{\beta}$ such that $\hat{y} = X \hat{\beta}$ is closest in Euclidean distance to $y$ (least squares).

We usually think about our data as $n$ data points in $p$ dimensional space e.g. we plot $y$ over one column of $X$, etc. Instead, for this intuition about the normal equations, we have to re-arrange our point of view to thinking about $y$ and $\hat{y}$ in $n$ dimensional space.

Think of $\hat{y} = X \hat{\beta}$ as an $n$-dimensional vector in the column space of $X$, written $C(X)$. This is hard for us to visualize when $n > 3$...

$\hat{y}$, being a linear combination of columns of X, must live in $C(X)$. We want to find the point in $C(x)$ that is closest (minimizing the distance, i.e. the squared residuals), to $y$. Another way to phrase this is that we want the residual vector, $\hat{\varepsilon} = y - \hat{y}$ to be orthogonal to $C(X)$.

We can write this requirement as:

$$
\begin{align}
X' (y - \hat{y}) &= \textbf{0} \\
X' (y - X \hat{\beta}) &= \textbf{0} \\
X' y - X' X \hat{\beta} &= \textbf{0} \\
X' y &= X' X \hat{\beta}
\end{align}
$$

which is the same system of linear equations we also found via calculus.

The matrix that projects $y$ to $\hat{y}$ in $C(X)$ is called $P$, the projection matrix.

Looking at the above, we can recognize that the projection matrix is defined by

$$
P = X (X' X)^{-1} X'
$$

which is perhaps familiar from linear algebra as the matrix which projects any vector $v$ onto the linear subspace spanned by the columns of $X$.

::: {.callout-note collapse="false"}
## Question

In fact the residuals $\hat{\varepsilon}$ are orthogonal to the column space of $X$, $C(X)$. Does this make sense? What if residuals were correlated with one of the columns of $X$?
:::

### 3D interpretation of normal equations

```{r}
#| eval: false
library(rgl)
y3d <- c(0,2,3)
X3d <- cbind(c(3,0,2),c(2,-3,1))
P <- X3d %*% solve(t(X3d) %*% X3d) %*% t(X3d)
yhat3d <- as.vector( P %*% y3d )
open3d()
bg3d(color = "white")
plot3d(
  y3d,
  xlim=c(-5,5), ylim=c(-5,5), zlim=c(-5,5),
  xlab="", ylab="", zlab=""
)
axes3d(labels = NULL)
normal <- pracma::cross(X3d[,1], X3d[,2])
planes3d(
  a = normal[1], b = normal[2], c = normal[3], 
  alpha = 0.5, col = "dodgerblue"
)
segments3d(rbind(c(0,0,0), X3d[,1]), col = "dodgerblue", lwd = 2)
segments3d(rbind(c(0,0,0), X3d[,2]), col = "dodgerblue", lwd = 2)
points3d(y3d, col = "red", size = 10)
points3d(yhat3d, col = "blue", size = 10)
segments3d(rbind(y3d, yhat3d), col = "black", lwd = 2)
text3d(y3d, texts = "y", pos = 3, col = "red")
text3d(yhat3d, texts = "Å·", pos = 3, col = "blue")
text3d(X3d[,1], texts = "x1", pos = 3, col = "dodgerblue")
text3d(X3d[,2], texts = "x2", pos = 3, col = "dodgerblue")
```

## Estimating the variance of "extra"

We will estimate the variance of the "extra" part of our data, $\textrm{Var}(\varepsilon_i) = \sigma^2$, with $\hat{\sigma}^2 = \frac{1}{(n-p)} \sum_i^{n} \hat{\varepsilon}_i$. The $n-p$ is familiar from the typical estimate of the sample variance, which takes into the "degree of freedom" that was used in estimating the mean.

::: {.callout-note collapse="false"}
## Question

It can be shown that $\textrm{Var}(\hat{\varepsilon}_i) = \sigma^2 (1 - h_i)$ with $h_i$ called the "hat value", equal to the i-th diagonal of the projection matrix $P$. This is not constant across observations $i$. Does this make intuitive sense, that while the $\varepsilon$ are assumed constant variance, the residuals are not? Note also that residuals are not independent while $\varepsilon$ are.
:::

## Properties of the OLS solution

The following are useful properties of our OLS solution $\hat{\beta}$:

-   It is unbiased, that is $E(\hat{\beta}) = \beta$.

-   It is "efficient", meaning it has the smallest variance of all unbiased estimators that are linear functions of the data. This is a result of the Gauss-Markov theorem, which has a good Wikipedia page with a proof of this result that you can consult. Briefly the steps are:

    1.  Take expectation of a linear estimator, and use unbiased property.
    2.  Take variance of a linear estimator using result from (1)

    If we add the assumption that $\varepsilon_i \sim N(0, \sigma^2)$ then we have that the OLS solution is the most efficient of *all* unbiased estimators.

-   The sampling distribution of $\hat{\beta}$ is approximately multivariate normal, with approximation better with large samples. With $\varepsilon_i \sim N(0, \sigma^2)$ then the sampling distribution is exactly multivariate normal.

-   If we add the assumption $\varepsilon_i \sim N(0, \sigma^2)$ then we can construct small-sample tests with the correct distribution (our statistics of interest will follow Student's $t$ distribution).

## Sampling variability of the OLS estimate

We can derive the sample covariance matrix for $\hat{\beta}$:

$$
\textrm{Cov}(\hat{\beta}) = \sigma^2 (X' X)^{-1}
$$

How does this look with uncorrelated predictors?

If we have columns of $X$, say $x^1$ and $x^2$ with correlation of 0, then their off-diagonal elements of $(X' X)^{-1}$ will also be 0. Hence their corresponding estimated coefficients will not be correlated. The sampling variability of elements of $\hat{\beta}$ will depend mostly on $n$ and the variance of the corresponding column of $X$. In fact it is the product of these terms (using the biased variance estimate).

```{r}
set.seed(5)
# define a function to get estimates of beta:
simulate_estimating_betas <- \(n, nreps) {
  x1 <- rep(0:1, each=n/2)
  x2 <- rep(0:1, times=n/2)
  X <- cbind(intercept=rep(1,n), x1, x2)
  b <- c(3, 2, 1)
  extra_sd <- 1
  # precomputed objects:
  X_times_b <- X %*% b
  Xt_X_inv_Xt <- solve(t(X) %*% X) %*% t(X)
  replicate(nreps, {
    extra <- rnorm(n, sd = extra_sd)
    y <- X_times_b + extra
    b_hat <- Xt_X_inv_Xt %*% y
    as.vector(b_hat)
  })
}
```

```{r}
set.seed(5)
n_values <- c(16, 40, 100, 500, 1000)
nreps <- 1000
est_betas <- lapply(
  n_values,
  simulate_estimating_betas,
  nreps=nreps
)
```

```{r}
est_beta1 <- lapply(est_betas, \(b) b[2,])
names(est_beta1) <- paste0("n=", n_values)
boxplot(est_beta1)
```

```{r}
(var_of_betas <- sapply(est_beta1, var))
plot(n_values, var_of_betas, type="b", log="xy")
# theoretical variance:
points(n_values, 4/n_values, type="b", col="blue")
```

```{r}
cor_beta1_beta2 <- sapply(est_betas, \(b) cor(t(b))[2,3])
plot(cor_beta1_beta2, ylim=c(-1,1))
```

How does this look with correlated predictors?

```{r}
library(mvtnorm)
simulate_estimating_betas_corX <- \(rho, nreps, n = 100) {
  replicate(nreps, {
    X <- mvtnorm::rmvnorm(n, 
      mean = c(0,0),
      sigma = cbind(c(1,rho),c(rho,1))
    )
    b <- c(2, 1)
    extra_sd <- 1
    X_times_b <- X %*% b
    Xt_X_inv <- solve(t(X) %*% X)
    extra <- rnorm(n, sd = extra_sd)
    y <- X_times_b + extra
    b_hat <- Xt_X_inv %*% t(X) %*% y
    c(cor(X)[1,2], as.vector(b_hat), as.vector(Xt_X_inv))
  })
}
```

```{r}
set.seed(5)
rho_values <- c(0, .25, .5, .75, .95, .99)
nreps <- 1000
est_cor_and_betas <- lapply(
  rho_values,
  simulate_estimating_betas_corX,
  nreps=nreps
)
```

```{r}
sample_cor <- lapply(est_cor_and_betas, \(x) x[1,])
names(sample_cor) <- paste0("r=", rho_values)
boxplot(sample_cor)
abline(h=rho_values, col=rgb(0,0,0,.3))
```

```{r}
est_beta1 <- lapply(est_cor_and_betas, \(x) x[2,])
names(est_beta1) <- paste0("r=", rho_values)
boxplot(est_beta1)
```

```{r}
var_beta1 <- sapply(est_cor_and_betas, \(x) var(x[2,]))
XtXinv_11 <- sapply(est_cor_and_betas, \(x) mean(x[4,]))
XtXinv_22 <- sapply(est_cor_and_betas, \(x) mean(x[7,]))
plot(rho_values, var_beta1)
points(rho_values, XtXinv_11, col="blue")
```

```{r}
cor_beta1_beta2 <- sapply(est_cor_and_betas, \(x) cor(t(x))[2,3])
XtXinv_12 <- sapply(est_cor_and_betas, \(x) mean(x[5,]))
plot(rho_values, cor_beta1_beta2)
points(rho_values, XtXinv_12 / sqrt(XtXinv_11 * XtXinv_22), col="blue")
```

::: {.callout-note collapse="false"}
## Question

What do these two results tell us about design of experiments and power to detect variance explained in $y$ by columns of $X$?
:::

## Analysis of Variance (ANOVA)

Does $X$ "explain" more variance in $y$ than we expect by chance?

We can look at the increase in the sample variance in $\hat{y}$ compared to the sample variance of $y$. The ratio of these variances is called $R^2$ and is often used to evaluate how well a model explains the data.

One caution is that $R^2$ is always increasing with additional columns of $X$. To get around this concern, we can either:

-   Use a test set to evaluate $R^2$ to know if our model generalizes well
-   Perform statistical tests which take into account how many columns of $X$ we used to explain variance in $y$.

The latter option is usually performed via t-test for one column of $X$ at a time, or F-test in general.

## Extra variance

When we look at the residuals, really this is just extra variance. It's misleading to call it "error" or "noise" although these are the common ways to refer to $\hat{\varepsilon}_i$. Let's consider two cases where the sources of extra variance are not well described as "error" or "noise".

We will switch from here on from the manual matrix computation of estimates to using R's built in linear model function `lm()`. We will use the *broom* package to show summary statistics from the linear model.

```{r}
set.seed(5)
n <- 100
x1 <- rnorm(n, sd=0.5)
x2 <- rnorm(n, sd=0.5)
X <- cbind(intercept=rep(1,n), x1, x2)
b <- c(5, 2, 2)
extra_sd <- 1
extra <- rnorm(n, sd=extra_sd)
y <- X %*% b + extra 
fit <- lm(y ~ 1 + x1 + x2)
library(broom)
tidy(fit) # the (X'X)-1 X'y estimates and s.e.
glance(fit) # more statistics
```

### 1. Effect of a missing predictor

A simple one, what if we leave out a predictor?

```{r}
fit_mis <- lm(y ~ 1 + x1)
tidy(fit_mis) 
glance(fit_mis)["sigma"]
```

::: {.callout-note collapse="false"}
## Question

Why did the standard errors increase for the intercept and x1 coefficient? Do you think they are biased?
:::

Let's do another case where x1 and x2 are correlated.

```{r}
rho <- .7
X0 <- mvtnorm::rmvnorm(
  n, mean = c(0,0), 
  sigma = cbind(c(1,rho),c(rho,1))
)
x1 <- X0[,1]
x2 <- X0[,2]
X <- cbind(intercept=rep(1,n), x1, x2)
b <- c(5, 2, 2)
extra_sd <- 1
extra <- rnorm(n, sd = extra_sd)
y <- X %*% b + extra
```

The second one is a mis-specified model. Note how far the true value is from the confidence interval!

```{r}
fit <- lm(y ~ 1 + x1 + x2)
tidy(fit)
fit_mis <- lm(y ~ 1 + x1)
tidy(fit_mis)
confint(fit_mis)
```

One can derive that this second estimator for the slope associated with $x^1$, $\hat{\beta}_1^{mis}$, will be biased by the amount $\rho \beta_2$. So the higher the correlation and the larger the true effect of $\beta_2$, the worse the bias will be.

### 2. Conditional effects

Simulate beta conditional on some value of an X variable

ATE, ATT, etc. link to Hernan book

## What about wiggliness?

Can we still use linear model? Yes!

There are a number of choices in R. You can specify polynomials (ideally with an orthogonal basis), or semi-parametric smooth curves using the splines package. Luckily, there are basis vectors which allow us to fit smoothing splines using the same linear model solution (OLS) described above. The same analysis of variance can be used.

## Bootstrap to estimate variability of estimates

Besides our formula above for estimating sampling variability of various quantities, which all relied on $\hat{\sigma}^2$, we can also use bootstrapping to estimate variances. Let's show an example, and compare the standard estimators to bootstrap estimators.

```{r}
set.seed(5)
n <- 100
x1 <- rnorm(n, sd=0.5)
x2 <- rnorm(n, sd=0.5)
X <- cbind(intercept=rep(1,n), x1, x2)
b <- c(5, 2, 2)
extra <- rt(n, df=2)
y <- X %*% b + extra 
```

Bootstrapping Regression Models in R

BCa bootstrap

```{r}
library(car)
fit <- lm(y ~ 1 + x1 + x2)
summary(fit)$coef
set.seed(5)
bfit <- Boot(fit, method="residual")
summary(bfit)
confint(fit)
Confint(bfit)[,-1]
```